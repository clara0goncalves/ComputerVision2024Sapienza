{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%mkdir yolov5_ws\n",
        "%cd yolov5_ws\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "u5qyhHzWtZvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import psutil  # For memory usage\n",
        "\n",
        "def run_inference_on_video(model_path, input_video, output_video, conf_threshold=0.1):\n",
        "    # Load YOLOv5 model\n",
        "    model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_path)\n",
        "\n",
        "    # Set model confidence threshold\n",
        "    model.conf = conf_threshold\n",
        "\n",
        "    # Open the input video file\n",
        "    video_capture = cv2.VideoCapture(input_video)\n",
        "    if not video_capture.isOpened():\n",
        "        print(\"Error: Could not open video.\")\n",
        "        return\n",
        "\n",
        "    # Get video properties\n",
        "    original_frame_width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    original_frame_height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(video_capture.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "    # Define codec and create VideoWriter object\n",
        "    video_writer = cv2.VideoWriter(output_video, cv2.VideoWriter_fourcc(*'mp4v'), fps, (original_frame_width, original_frame_height))\n",
        "\n",
        "    # Initialize metrics\n",
        "    start_time = time.time()\n",
        "    frame_count = 0\n",
        "    total_inference_time = 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = video_capture.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "        # Preprocess frame for YOLOv5 (resize to 640x640)\n",
        "        resized_frame = preprocess_frame(frame)\n",
        "\n",
        "        # Measure inference time\n",
        "        inference_start = time.time()\n",
        "        results = model(resized_frame)\n",
        "        inference_end = time.time()\n",
        "\n",
        "        # Update total inference time\n",
        "        total_inference_time += (inference_end - inference_start)\n",
        "\n",
        "        # Get bounding boxes and annotations\n",
        "        boxes = results.xyxy[0].cpu().numpy()  # Get boxes as numpy array\n",
        "\n",
        "        # Post-process boxes to original size\n",
        "        boxes_rescaled = postprocess_boxes(boxes, (original_frame_height, original_frame_width), (640, 640))\n",
        "\n",
        "        # Draw bounding boxes on the original frame\n",
        "        for box in boxes_rescaled:\n",
        "            x1, y1, x2, y2, conf, cls = map(int, box)\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "            label = f'{model.names[cls]} {conf:.2f}'\n",
        "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "        # Perform lane detection\n",
        "        frame_with_lanes = detect_lanes(frame)\n",
        "\n",
        "        # Write the processed frame with both object detection and lane detection to the output video\n",
        "        video_writer.write(frame_with_lanes)\n",
        "\n",
        "        # Calculate and print status metrics every 10 frames\n",
        "        if frame_count % 10 == 0:\n",
        "            elapsed_time = time.time() - start_time\n",
        "            avg_inference_time = total_inference_time / frame_count\n",
        "            fps_current = frame_count / elapsed_time\n",
        "            memory_usage = psutil.Process().memory_info().rss / (1024 ** 2)  # Memory usage in MB\n",
        "\n",
        "            print(f\"FPS: {fps_current:.2f}, Average Inference Time: {avg_inference_time:.4f} seconds, Memory Usage: {memory_usage:.2f} MB\")\n",
        "\n",
        "    # Release video objects\n",
        "    video_capture.release()\n",
        "    video_writer.release()\n",
        "\n",
        "    print(f\"Output video saved to: {output_video}\")\n",
        "\n",
        "# Define paths to model, input video, and output video\n",
        "model_path = '/content/destilation_prunned_quantized_fixed.onnx'  # Path to the YOLOv5 model (PyTorch format)\n",
        "input_video_path = '/content/test_video.mp4'  # Path to your input video\n",
        "output_video_path = '/content/output_video.mp4'  # Path to save the output video\n",
        "\n",
        "run_inference_on_video(model_path, input_video_path, output_video_path, conf_threshold=0.3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7APvY5pztAKo",
        "outputId": "04094a70-31c3-43ec-f342-1a375684ab59"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 ðŸš€ 2024-9-4 Python-3.10.12 torch-2.4.0+cu121 CPU\n",
            "\n",
            "Loading /content/destilation_prunned_quantized_fixed.onnx for ONNX Runtime inference...\n",
            "Adding AutoShape... \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FPS: 3.28, Average Inference Time: 0.2943 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 2.77, Average Inference Time: 0.3497 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 2.95, Average Inference Time: 0.3282 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 3.06, Average Inference Time: 0.3170 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 3.12, Average Inference Time: 0.3100 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 3.00, Average Inference Time: 0.3231 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 3.03, Average Inference Time: 0.3198 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 3.08, Average Inference Time: 0.3154 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 3.11, Average Inference Time: 0.3119 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 2.99, Average Inference Time: 0.3239 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 2.78, Average Inference Time: 0.3486 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 2.81, Average Inference Time: 0.3458 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 2.82, Average Inference Time: 0.3447 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 2.81, Average Inference Time: 0.3458 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 2.84, Average Inference Time: 0.3416 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 2.87, Average Inference Time: 0.3381 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 2.89, Average Inference Time: 0.3362 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 2.86, Average Inference Time: 0.3388 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 2.89, Average Inference Time: 0.3360 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 2.91, Average Inference Time: 0.3333 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 2.93, Average Inference Time: 0.3310 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 2.90, Average Inference Time: 0.3342 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 2.92, Average Inference Time: 0.3320 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 2.94, Average Inference Time: 0.3300 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 2.96, Average Inference Time: 0.3281 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 2.93, Average Inference Time: 0.3309 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 2.95, Average Inference Time: 0.3292 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 2.96, Average Inference Time: 0.3276 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 2.97, Average Inference Time: 0.3261 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 2.95, Average Inference Time: 0.3282 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 2.96, Average Inference Time: 0.3272 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 2.98, Average Inference Time: 0.3259 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 2.99, Average Inference Time: 0.3246 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 2.98, Average Inference Time: 0.3258 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 2.98, Average Inference Time: 0.3256 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 2.99, Average Inference Time: 0.3244 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 3.00, Average Inference Time: 0.3234 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 2.99, Average Inference Time: 0.3239 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 2.99, Average Inference Time: 0.3245 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 3.00, Average Inference Time: 0.3235 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 3.01, Average Inference Time: 0.3226 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 3.01, Average Inference Time: 0.3226 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 3.00, Average Inference Time: 0.3236 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 3.01, Average Inference Time: 0.3227 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 3.01, Average Inference Time: 0.3218 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 3.02, Average Inference Time: 0.3213 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 3.00, Average Inference Time: 0.3228 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 3.01, Average Inference Time: 0.3220 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 3.02, Average Inference Time: 0.3213 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 3.03, Average Inference Time: 0.3205 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 3.01, Average Inference Time: 0.3222 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 3.02, Average Inference Time: 0.3215 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 3.02, Average Inference Time: 0.3207 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 3.03, Average Inference Time: 0.3200 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 3.02, Average Inference Time: 0.3216 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 3.02, Average Inference Time: 0.3209 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 3.03, Average Inference Time: 0.3203 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 3.03, Average Inference Time: 0.3197 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 3.02, Average Inference Time: 0.3209 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 3.03, Average Inference Time: 0.3204 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 3.03, Average Inference Time: 0.3197 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 3.04, Average Inference Time: 0.3192 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 3.03, Average Inference Time: 0.3200 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 3.03, Average Inference Time: 0.3199 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 3.04, Average Inference Time: 0.3193 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 3.04, Average Inference Time: 0.3188 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 3.04, Average Inference Time: 0.3192 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 3.03, Average Inference Time: 0.3195 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 3.04, Average Inference Time: 0.3190 seconds, Memory Usage: 945.93 MB\n",
            "FPS: 3.04, Average Inference Time: 0.3185 seconds, Memory Usage: 945.93 MB\n",
            "Output video saved to: /content/output_video.mp4\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}